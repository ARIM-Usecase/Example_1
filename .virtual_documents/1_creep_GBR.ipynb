








!git clone https://github.com/ARIM-Usecase/Example_1.git
%cd Example_1





# 汎用ライブラリ
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# 機械学習
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split 
from sklearn.ensemble import GradientBoostingRegressor

from sklearn import datasets, metrics
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

# XAI
import shap





# READ DATASET
df = pd.read_excel('./data/creep_data.xlsx')  
df





# Remove outliers from dataset
z_scores = np.abs(stats.zscore(df))
df_clean = df[(z_scores < 3).all(axis=1)]


df_clean











# Plot Pearson Correlation heatmap
plt.figure(figsize=(8, 8))
plt.title('Pearson Correlation of features')

sns.heatmap(df_clean.corr(), 
            linewidths=1.0, 
            vmax=1.0, 
            square=True, 
            cmap="RdYlBu", 
            linecolor='white', 
            annot=False
           )

sns.set(font_scale=1.0)
plt.xticks(fontsize=10, rotation=90)
plt.yticks(fontsize=10, rotation=0, va="center")

plt.show()





# Feature selection using PCC threshold
threshold = 0.80
corr = df_clean.corr()

# Identify and remove highly correlated features
to_drop = [col for i, col in enumerate(corr.columns) 
           if any(abs(corr.iloc[i, :i]) >= threshold)]

df_fin = df_clean.drop(columns=to_drop)

print("-------After removing highly correlated features------------------------------")
print(df_fin.columns)  # final feature set





df_fin





X = df_fin[[ 'Al', 'V', 'Fe', 'Mo', 'N',
        'Solution treated temp(cel)', 'Anneal temp (cel)',
        'Annealing Time (hour)', 'Temperature of creep test (cel)',
        'Stress (Mpa)', 'steady state strain rate (1/s)',
        'Strain to rupture (%) (Efc)']]                                
y = np.log10(df_fin.iloc[:,0]) 


X





# Standardization module
def stand(df1, df2):
    scaler = StandardScaler().fit(df1)
    return pd.DataFrame(scaler.transform(df2), columns=df2.columns)





X_train,X_test,y_train,y_test = train_test_split(X,y, 
                                                 test_size=0.1,
                                                 random_state=3000
                                                )                                      


X_test=stand(X_train,X_test)
X_train=stand(X_train,X_train) 


X_train


X_test





reg = GradientBoostingRegressor(loss='squared_error', learning_rate=0.01,
                            n_estimators=1600, subsample=1.0, max_depth=3,
                            criterion='friedman_mse', min_samples_split=2,
                            min_samples_leaf=1, min_weight_fraction_leaf=0.0,
                            random_state=3000)
 
reg.fit(X_train,y_train)








#予測値の計算
predicted_test = reg.predict(X_test)
predicted_train = reg.predict(X_train)


#モデル性能の評価
R2_test = metrics.r2_score(y_test, predicted_test)
R2_train = metrics.r2_score(y_train, predicted_train)

rmse_test = np.sqrt(metrics.mean_squared_error(y_test, predicted_test))
rmse_train = np.sqrt(metrics.mean_squared_error(y_train, predicted_train))


print ("Output of model")
print ("R2 train/test =  ",R2_train,"/",R2_test)
print ("RMSE train/test =  ",rmse_train,"/",rmse_test)





fig, ax = plt.subplots(figsize=(10, 10))

# Plot Train data
plt.scatter(y_train, reg.predict(X_train), s=400, marker='s', linewidth=2,
            edgecolors="blue", label='Train data', color="lavender")

# Plot Test data
plt.scatter(y_test, reg.predict(X_test), s=500, marker='s', linewidth=2,
            edgecolors="white", label='Test data', color="red")

# Plot diagonal line
plt.plot([-2, 4], [-2, 4], linestyle='dashed', color='black', linewidth=3.5)

# Customize plot
plt.legend(loc='upper left', fontsize='xx-large', facecolor='w', edgecolor='black')
plt.tick_params(axis='both', which='major', labelsize=18)
plt.xlabel('Actual', fontsize=20)
plt.ylabel('Predicted', fontsize=20)
plt.show()





# SHAP feature importances
shap_values = shap.TreeExplainer(reg).shap_values(X_train)








shap.summary_plot(shap_values, features = X_train, show=False)     








##----bar plot  
shap.summary_plot(shap_values, X_train, plot_type = "bar")  





shap_values = shap.TreeExplainer(reg).shap_values(X_train)
for i in range(12):
    shap.dependence_plot(i, shap_values, X_train,show=False,
                      interaction_index='auto', dot_size=200)  



